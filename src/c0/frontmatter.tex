%--------------------------- pautoreface 2 ---------------------------------
\phantomsection
\addcontentsline{toc}{section}{第2版序}
\section*{第2版序}\label{sec:pautoreface_2}

由本书的第一版出版至今的二十年见证了人工智能领域的巨大进步, 该进步很大程度上是由机器学习的发展推动的, 而机器学习的发展中也包括了强化学习的发展. 虽然计算能力的增强为这些发展中一部分做出了贡献, 但理论与算法上的新进展同样是强大的驱动力. 面对这样的进步, 相较于1998年版本的再版刻不容缓, 于是我们终于在2012年开始了这个项目. 本书第二版的目标同第一版是一致的: 向所有关联领域的读者, 提供清晰明了的强化学习关键概念与算法. 本版本依然是导论性质的, 并且我们仍将注意力集中于核心的在线<online>学习算法. 本版本包含了一些在间隔的这些年中重要性日渐凸显出来的主题, 并且我们拓宽了对我们现在理解得更好的主题的涵盖. 但我们不会试图提供对强化学习领域全面的覆盖, 因为其在许多不同的方向上有爆炸式的发展. 我们为不得不遗漏这些发展中的大部分而只能保留小部分而道歉.

就像在第一版中做的那样, 我们选择不以严密、正式的方式对强化学习进行阐述, 也不在最一般的情形下进行阐述. 然而, 自第一版以来我们在一些主题上形成了的更为深入的理解, 这些理解需要稍多的数学来解释; 我们将需要更多数学知识的部分用带阴影的方框隔开, 以便苦于数学的读者可以选择跳过. 我们也使用了和第一版略微不同的数学标记. 在教学的过程中, 我们发现新的标记系统可以解决一些普遍的疑惑. 这套标记系统强调了随机变量与其实例的区别, 其中前者标记为大写字母, 后者标记为小写字母. 举个例子, 时步$t$中的状态<state>, 动作<action>与奖赏<reward>分别被记为$S_t$, $A_t$与$R_t$,而它们可能的取值可以被记为$s$, $a$, $r$. 类似地, 可以很自然地将小写字母用于值函数<value function>(比如$v_{\pi}$), 并将大写字母限定于它们表格式<tabular>的估计值.  近似值函数<approximate value function>是任意参数的确定性<deterministic>函数, 因此也使用小写字母标记(例如$\hat v (\mathbf s, \mathbf w_t) \approx v_\pi(\mathbf s)$). 向量, 例如权重向量$\mathbf w_t$(或更为正式的${\boldsymbol \theta}_t$)以及特征向量$\mathbf x_t$(或更为正式的$\boldsymbol \phi_t$), 都是用粗体的小写字母标记, 即使它们为随机变量. 大写的粗体为矩阵保留. 在第一版中我们使用特定的标记$\mathfrak P_{s, s^\prime}^a$和$\mathfrak R_{s, s^\prime}^a$来分别标识转移概率与奖赏的期望值. 这两个标记的一个缺点是其仍然不能完全描述奖励的动态<dynamics>, 因为其只给出了奖励的期望值: 这对动态规划<dynamic programming>而言是足够了, 但对强化学习而言远远不够. 这两个标记的另一个缺点是对下标与上标的滥用. 在本版中我们使用$p(s', r \mid s, a)$这一明确的标记来表示给定当前状态与动作的情况下, 下一状态与奖赏的联合概率. 所有标识的变化都列于\hyperlink{sec:notation}{标识一览}中.

本版有了极大的拓展, 并且顶层的组织发生了改变. 在介绍性质的第一章后, 本版被划分为三个新部分. \partref{1}(\chapref{2}到\chapref{8})尽可能多地介绍表格式情形下的强化学习, 在该情形下我们可以获得真实解. 我们涵盖了在表格式情形下的学习<learning>与计划<planning>方法, 以及两者在$n$-步<$n$-step>方法及Dyna中的统一. 许多呈现在这一部分的算法是第二版新增的, 包括UCB, 期望Sarsa<Expected Sarsa>, 双重学习<Double Learning>, 树逆流<tree-backup>, $Q(\sigma)$, RTDP以及MTCS. 先在表格式的情况下进行彻底的展开, 使得我们能在最简单的设定下进行建立起核心概念. 本书的\partref{2}(\chapref{9}到\chapref{13})致力于将这些概念拓展到函数近似<function approximation>的情况下. 其新增了关于以下内容的新的章节: 人工神经网络, 傅里叶基函数, LSTD, 核方法, 梯度-TD<Gradient-TD>与强调-TD<Emphatic-TD>方法, 平均奖励<average-reward>方法, 真在线TD($\lambda$) <true online TD($\lambda$)>, 以及策略梯度<policy-gradient>方法. 本版极大地拓展了异策略学习<off-policy learning>的内容, 首先是第5到7章中的表格式的情形, 其次是在第11, 12章中的使用函数近似的情形. 本版中的另一个变化是将$n$-步自举<n-step bootstrapping>中的前向视角<forward-view>的概念(现在在\chapref{7}中被更详细地阐述), 与资格迹<eligibility trace>中的后向视角<backward-view>的概念(现在在\chapref{12}中独立阐述)分离开来. 本书的\partref{3}增添了关于以下内容的大量新章节: 强化学习与心理学(\chapref{14})和神经科学(\chapref{15})的关联, 以及包括了雅达利游戏、Watson的赌博策略以及围棋程序AlphaGo与AlphaGo Zero的更新后案例学习章节(\chapref{16}). 像上版那样, 由于必要性使然, 我们仅覆盖了强化学习领域中所有成就的一小部分. 我们的选择折射出我们对于计算上廉价的无模型<model-free>方法的长期兴趣, 同时其也能够很好地被拓展到大型应用上. 最后一章现在包含了关于强化学习未来的社会影响的讨论. 不论好坏, 本版大约是第一版的两倍长.

本书被设计为用于一到两学期的强化学习课程的基本参考书. 对于一学期的课程, 应该涵盖前十章以形成一个良好的核心, 在这之上可以根据教学口味添加其他章节的材料, 也可以添加来自其他参考书, 如\citet{Bertsekas1996}, \citet{Wiering2012a}与\citet{Szepesvari2010}的材料, 或者可以添加来自文献的材料. 视学生的背景而定, 一些额外的在线监督学习的材料可能是会有帮助的. 选择<option>与选择模型<option model>的概念也是一个自然的附加材料(\citet{Sutton1999}). 两学期的课程可以涵盖所有的章节并补充材料. 本书也可作为更宽泛的课程——如机器学习、人工智能及人工神经网络——的一部分. 在这种情况下, 可能只需要涵盖本书的部分内容. 我们推荐涵盖\chapref{1}作为简明的概率, 涵盖\chapref{2}中直到\secref{2.4}(包括)中的内容, 以及涵盖\chapref{3}, 然后根据时间与兴趣从余下部分选择章节. \chapref{6}是对整个课题与余下内容而言最为重要的部分. 专注于机器学习或人工神经网络的课程应该涵盖\chapref{9}与\chapref{10}, 而专注于人工智能或规划的课程应该涵盖\chapref{8}. 在整本书中, 更为困难且对余下内容并非必要的章节被标上了*. 这些内容可以在首次阅读的时候跳过, 而不会对阅读后续的内容产生影响. 一些练习题也被标上了*以显示其更为深入, 并且对理解那一章的基本内容不是必需的. 

大多数的章以题为``参考文献与历史沿革''的节结束, 在其中我们阐明该章中出现的概念的源头, 提供对更深入的阅读与正在进行的研究的指引, 并且叙述相关的历史背景. 虽然我们想使这些节权威与完整, 但我们毫无疑问地遗漏了一些重要的先驱性工作. 对此我们再次致以歉意, 同时我们欢迎指正与拓展, 并且乐于将其并入本书的电子版中.

像第一版一样, 仅以本书的第二版缅怀A. Harry Klopf. 正是Harry将我们介绍给彼此, 同时正是他关于大脑与人工智能的想法开启了我们走近强化学习的漫长旅途. Harry接受过神经生理学的学习并且长期地对机器智能拥有兴趣, 是一位隶属于位于俄亥俄州Wright-Patterson空军基地的空军研究所航电部门<Avionics Directorate of the Air Force Office of Scientific Research, AFOSR>的资深科学家. 他不满于在解释自然智能与为机器智能提供基础方面, 巨大的重要性被赋给了平衡寻求<equilibrium-seeking>过程, 包括内平衡与误差纠正的模式分类法. 他指出试图最大化某个量(无论这个量是什么)的系统与平衡寻求系统有本质上的区别, 并且他主张最大化系统是理解自然智能的某些重要方面的关键所在, 也是构建人工智能系统的关键所在. Harry促成了从AFOSR处获得资金, 来开展项目评估上述及相关观点的科学价值. 这个项目在于70年代晚期在UMass Amherst展开, 最初由Michael Arbib, William Kilmer以及Nico Amherst指导, 他们既是UMass Amherst的计算机与信息科学系的教授, 同时也是大学中系统神经科学相关神经机械学中心的创始人, 共同组成了一个专注于神经科学与人工智能的交叉领域的、有远见的团队. Barto, 作为刚毕业于密歇根大学的Ph.~D., 被聘请为项目的博士后研究员. 与此同时, Sutton, 作为一个学习计算机科学与心理学的斯坦福大学本科生, 那时曾一直与Harry就``刺激的时序<timing>在经典条件作用<classical conditioning>中扮演的角色''这一共同兴趣进行通信. Harry向UMass的团队建议Sutton也应该加入到项目中. 因此Sutton成为了UMass的研究生, 他的博士导师是已经成为副教授的Sutton. 呈现在本书中的关于强化学习的研究, 正可以说是这个由Harry发起并受到他的思想启发的项目的产物. 更进一步讲, 正是Harry将我们, 两位作者, 带入到了一段长而令人享受的友谊中. 仅以此书献给Harry以纪念他必不可少的贡献, 这贡献不仅是对强化学习领域的, 也是对我们友谊的. 我们也要感谢Arbib教授, Kilmer教授以及Spinelli教授为我们提供了探索这些想法的机会. 最后, 我们要感谢AFOSR在对我们早年的研究的慷慨援助, 以及NSF在此后的许多年中的慷慨援助.

我们必须感谢许多人, 感谢他们为这本版所提供的启示与帮助. 为第一版提供启示与帮助的、我们曾致谢的每一个人, 同样也值得我们在本版中致以最深沉的感谢, 因为如果没有他们对第一版所做的贡献的话, 本版也就不会存在了. 此外, 我们也必须将许多为第二版做出了特别的贡献的人, 加入到致谢列表中. 过去这些年使用这份材料教出来的学生, 在无数方面做出了贡献: 暴露出书中的错误, 提供对错误的改正, 以及同样重要的, 提出困惑来指明我们本可以解释得更好. 我们特别感谢Martha Steenstrup, 因为他从头至尾地阅读本书并提供了详尽的评论. 如果没有心理学与神经科学领域的许多专家的帮助的话, 关于这些内容的章节是不可能完成的. 我们向John Moore致谢, 为他在许许多多年中关于动物学习的实验、理论以及神经科学的耐心指导, 也为他对\chapref{14}与\chapref{15}的许多草稿的仔细阅读. 我们同样也要感谢Matt Botvinick, Nathaniel Daw, Peter Dayan和Yael Niv, 为他们在这些章节的草稿上所提供的富有洞察力的评论, 为他们在大量的文献上提供的必不可少的指导, 以及为他们对我们早期的草稿中许多错误的拦截. 当然, 这些章节中遗留的错误——一定还遗留了一些——都应该归到我们头上. 我们感谢Phil Thomas, 因为他帮助我们使这些章节能被非心理学与神经科学专业的读者理解; 我们也感谢Peter Sterling, 因为他帮助我们改善了论述. 我们对Jim Houk充满了感激之情, 因为他向我们介绍了基底核<basal ganglia>中信息处理的内容, 也在神经科学的相关方面向我们做出提醒. José Martínez, Terry Sejnowski, David Silver, Gerry Tesauro, Georgios Theocharous以及Phil Thomas帮助我们理解他们的强化学习应用中的细节, 以便这些内容能被包含在``案例学习''这一章中, 他们也为这些章节的草稿提供了建设性的意见. 特别的感谢被致以David Silver, 因为他帮助我们更好地理解了MCTS与DeepMind的围棋程序. 我们感谢George Konidaris在傅里叶基函数这一节中提供的帮助. 我们感谢Emilio Cartoni, Thomas Cederborg, Stefan Dernbach, Clemens Rosenbaum, Patrick Taylor, Thomas Colin, and Pierre-Luc Bacon, 为他们在一些我们所感激的重要方面所提供的帮助.

Sutton也想要感谢University of Alberta的强化学习与人工智能实验室的同事, 为他们对本版做出的贡献. Sutton必须特别感谢Rupam Mahmood, 为他在\chapref{5}的异策略<off-policy>蒙特卡洛方法方面做出的贡献; 感谢Hamid Maei, 为他对\chapref{11}呈现的异策略学习的更进一步的观点提供的帮助;  感谢Eric Graves, 为\chapref{13}中他所做的的实验; 感谢Shangtong Zhang, 为他对几乎所有的实验的重做与对实验结果的验证; 感谢Kris De Asis, 为他在\chapref{7}与\chapref{12}中的对关于新技术的内容的所做出的改进; 感谢Harm van Seijen, 为他将$n$-步方法与资格迹分离开来的洞察力, 以及(和Hado van Hasselt一起)所提供的呈现在\chapref{12}中关于资格迹的前向视图与后向视图的等价性的观点. Sutton也要感谢Alberta政府与加拿大国家科技研究议会<National Science and Engineering Research Council of Canada>在本版的构思与书写期间提供的支持与自由. Sutton同样也特别感激Randy Goebel为Alberta所创造的富有远见与支持性的学术环境. Sutton也要感谢DeepMind在本书书写的最后6个月所提供的帮助.

最后, 我们要感谢许许多多的本版在线草稿的读者. 他们发现了我们遗漏的很多错误, 也就一些潜在的困惑点向我们发出警告.
\clearpage

% --------------------------------- preface 1 --------------------------------
\phantomsection
\addcontentsline{toc}{section}{第1版序}
\section*{第1版序}\label{sec:pautoreface_1}


我们最早在1979年末关注于现在被称为强化学习的学科. 我们在University of Massachusetts, 工作于最早的想要使``类似神经元的自适应元素组成的网络, 可能是通往自适应人工智能的一条充满前景的道路''这一理念复兴的项目. 这一项目探索了由A. Harry Klopf发展的``自适应系统的多稳态<heterostatic>理论''. Harry的工作是许多想法的丰富源泉, 因而我们能够批判性地探索这些想法, 并将它们和拥有长久历史的关于自适应系统的先驱工作相比较. 我们的工作变成了这两样: 将这些想法梳理开来, 或理解这些想法的关系与相对的重要性. 这一直持续到今天, 但在1979年我们突然意识到这其中最简单的想法, 也是长久以来一直被认为是理所当然的想法, 几乎没有从计算的角度受到过关注. 这个简单的想法就是: 学习系统\emph{想要}一些东西, 或学习系统为了从环境中最大化某一个特殊的信号而适应性地改变行为. 这就是``享乐主义''<hedonistic>学习系统的理念, 或者用现在的话说, 强化学习的理念.

在那时, 我们像其他人那样有一种这样的感觉: 强化学习已经在神经机械学与人工智能发展的早期被彻底地探索过了. 然而在更近地审视后, 我们发现这探索仅是浅显的. 虽然强化学习很明显地催生了一些最早的对学习理论的计算方面的研究, 但大多数研究者转向了其他事物: 例如模式分类, 监督学习, 自适应控制理论, 或者干脆放弃了学习理论的研究. 结果涉及到``怎样从环境中获得某物''的特定问题相对而言几乎没有受到关注. 回想起来, 对这一理念的关注, 是使得这一分支的研究蓬勃发展的决定性的一步. 如果没有察觉到这样一个基本的理念还没有被彻底探索过的话, 关于强化学习的计算方面的研究是不可能开展起来的.

从那时起强化学习领域已经有了长足的发展, 并在几个方面上逐渐进步与成熟. 强化学习逐渐成为了机器学习、人工智能以及神经网络方面的最为活跃的研究领域. 这一领域发展出了坚实的数学基础与令人印象深刻的应用. 强化学习的计算方面的研究现在是一个宽广的领域, 有来自全球的数以百计的心理学、控制论、人工智能与神经科学等领域的活跃研究者参与其中. 其中关于``建立与发展最优控制理论与动态规划这两者关系''的贡献是至关重要的. 从交互中学习以达成目标这一整体问题还远没有解决, 但我们对它的理解已经极大地改善了. 我们现在能将时序差分<temporal-difference>学习、动态规划、函数近似等组成概念聚合到关于这整个问题的统一视图中. 

我们书写这本书的目的, 是为了提供一个清晰明了的对强化学习的关键概念与算法的阐述. 我们希望我们的阐述能被关联领域的读者理解, 但我们不会详尽地涵盖所有这些领域. 就绝大部分而言, 我们的阐述是从人工智能与工程的角度出发的. 对其他关联领域的涵盖, 我们要么留给其他作者, 要么以后再论. 我们也选择不以严谨而正式的方式阐述强化学习. 我们没有到达可能的最高级别的数学抽象, 也没有依赖于定理-证明<theorem-proof>体系. 我们试图选择呈现特定级别的的数学细节: 这样程度的细节既能指出正确的数学方向, 又能避免破坏根本概念的简洁性与潜在的普遍性.

(后为致谢内容, 暂不译)
\clearpage

% --------------------------------- notation --------------------------------
\phantomsection
\addcontentsline{toc}{section}{标识一览}
\hypertarget{sec:notation}{}
\section*{标识一览}\label{sec:notation}

大写字母用于随机变量, 反之小写字母用于随机变量的具体值或标量函数. 小写、粗体的字母用于实数向量(即使是随机变量). 大写的粗体字母用于矩阵.
\bigskip

\begin{longtable}[l]{p{6em}l}
$\doteq$  &  由定义得到的等于关系    \\
$\approx$ & 约等于 \\
$\propto$     &   正比于   \\
$\Pr \{X=x\}$     &    随机变量$X$取值为$x$的概率  \\
$X \sim p$   &   随机变量$X$满足分布$p(x) \doteq \Pr\{X = x\}$    \\
$\mathbb{E}[X]$    &   随机变量$X$的期望值, 也就是说$\mathbb{E}[X] = \sum_x p(x)x$   \\
$\targmax_a f(a)$   &   当$f(a)$取最大值时$a$的取值   \\
$\ln (x)$   &    $x$的自然对数   \\
$e^x$   &    自然常数$e \approx 2.71828$的$x$次方  \\
$\mathbb{R}$ & 实数集 \\
$f: \mathcal{X} \rightarrow \mathpzc{y}$ & 函数$f$表示从集合$\mathcal X$中元素到集合$\mathpzc{y}$中元素的映射 \\
$\leftarrow$ & 赋值 \\
$(a, b]$ & 左开右闭的实数区间 \\
 & \\
$\varepsilon$ & 在$\varepsilon$-贪心策略中采取随机动作的概率 \\
$\alpha, \beta$ & 步长参数 \\
$\gamma$ & 折扣率参数 \\
$\lambda$ & 资格迹中的衰减率 \\
$\mathbbm{1}_{predicate}$ & 指示函数(当谓词$predicate$为真时$\mathbbm{1}_{predicate} \doteq 1$, 反之为0) \\
 & \\
\end{longtable}


\noindent 在多摇臂赌博机问题中:

\begin{longtable}[l]{p{6em}l}
$k$      & 动作(摇臂)的数量         \\
$t$      & 离散的时步数或玩的次数     \\
$q_*(a)$ & 动作$a$的真实值(期望奖赏)\\
$Q_t(a)$ & $q_*(a)$在时步$t$的估计值       \\
$N_t(a)$ & 在时步$t$前动作$a$被选中的概率    \\
$H_t(a)$ & 由学习得到的、在时步$t$时选择动作$a$的偏好值 \\
$\pi_t(a)$ & 在时步$t$选择动作$a$的概率 \\
$\bar{R}_t$ & 在给定策略$\pi_t$的情况下, 期望奖赏在时步$t$时的估计值 \\
 & \\
\end{longtable} 

\noindent 在马尔科夫决策过程中:

\begin{longtable}[l]{p{6em}l}
$s, s'$ & 状态 \\
$a$ & 一动作 \\
$r$ & 一奖赏 \\
$\mathcal{S}$ & 所有非末状态的集合 \\
$\mathcal{S}_+$ & 所有状态的集合, 包括末状态 \\
$\mathcal{A}(s)$ & 在状态$s$下所有可行的动作的集合 \\
$\mathcal{R}$ & 所有可能奖赏的集合, 为$\mathbb{R}$的有限子集 \\
$\subset$ & 含于, 例如$\mathcal{R} \subset \mathbb{R}$ \\
$\in$ & 属于, 例如$s \in \mathcal{S}$, $r \in \mathcal{R}$ \\
$\lvert \mathcal{S} \rvert$ & 集合$\mathcal{S}$中元素的个数 \\
 & \\
$t$ & 离散的时步 \\
$T, T(t)$ & 分节的最后一个时步, 或包含了时步$t$的分节的最后一步 \\
$A_t$ & 在时步$t$中所选择的动作 \\
$S_t$ & 时步$t$时的状态, 通常由$S_{t - 1}$和$A_{t - 1}$概率性地决定 \\
$R_t$ & 在时步$t$中的奖赏, 通常由$S_{t - 1}$和$A_{t - 1}$概率性地决定 \\
$\pi$ & 策略(决策准则) \\
$\pi(s)$ & 在\emph{确定性}策略$\pi$下, 在状态$s$中所采取的动作 \\
$\pi(a \mid s)$ & 在\emph{概率性}策略$\pi$下, 在状态$s$中采取动作$a$的概率 \\
 & \\
$G_t$ & 在时步$t$后的回报 \\
$\dots$ & $\dots$ \\
 & \\
$p(s', r \mid s, a)$ & 从状态$s$与动作$a$起, 以$r$的奖赏转移到状态$s'$的概率 \\
$p(s' \mid s, a)$ & 从状态$s$和动作$a$起, 以转移到状态$s'$的概率 \\
$r(s, a)$ & 在状态$s$中采取动作$a$后获得的立即奖赏的期望值 \\
$r(s, a, s')$ & 在状态$s$中采取动作$a$后转移到状态$s'$所获得的立即奖赏的期望值 \\
 & \\
$v_\pi(s)$ & 在策略$\pi$下状态$s$的值(期望回报) \\
$v_*(s)$ & 在最优策略下状态$s$的值 \\
$q_\pi(s, a)$ & 在策略$\pi$下, 在状态$s$中采取动作$a$的值 \\
$q_*(s, a)$ & 在最优策略下, 在状态$s$中采取动作$a$的值 \\
 & \\
$V, V_t$ & 状态值函数$v_\pi$或$v_*$的表格式估计值 \\
$Q, Q_t$ & 动作值函数$q_\pi$或$q_*$的表格式估计值 \\
$\bar{V}_t(s)$ & 期望的近似动作值, 如$\bar{V}_t(s) = \sum_a \pi(a \mid s) Q_t(s, a)$ \\
$\dots$ & $\dots$
\end{longtable}